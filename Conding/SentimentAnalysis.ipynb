{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import nltk\n",
    "\n",
    "#from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn1</th>\n",
       "      <th>turn2</th>\n",
       "      <th>turn3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't worry  I'm girl</td>\n",
       "      <td>hmm how do I know if you are</td>\n",
       "      <td>What's ur name?</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did I?</td>\n",
       "      <td>saw many times i think -_-</td>\n",
       "      <td>No. I never saw you</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>By</td>\n",
       "      <td>by Google Chrome</td>\n",
       "      <td>Where you live</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U r ridiculous</td>\n",
       "      <td>I might be ridiculous but I am telling the truth.</td>\n",
       "      <td>U little disgusting whore</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just for time pass</td>\n",
       "      <td>wt do u do 4 a living then</td>\n",
       "      <td>Maybe</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   turn1                                              turn2  \\\n",
       "0  Don't worry  I'm girl                       hmm how do I know if you are   \n",
       "1            When did I?                         saw many times i think -_-   \n",
       "2                     By                                   by Google Chrome   \n",
       "3         U r ridiculous  I might be ridiculous but I am telling the truth.   \n",
       "4     Just for time pass                         wt do u do 4 a living then   \n",
       "\n",
       "                       turn3   label  \n",
       "0            What's ur name?  others  \n",
       "1        No. I never saw you   angry  \n",
       "2             Where you live  others  \n",
       "3  U little disgusting whore   angry  \n",
       "4                      Maybe  others  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"bun-data//train.txt\"\n",
    "data = pd.read_csv(path,  sep='\\t', lineterminator='\\r')\n",
    "data.drop(\"id\",axis=1,inplace=True)\n",
    "data.dropna(how = \"all\",inplace=True)\n",
    "data.fillna(\" \",inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "import re\n",
    "import spacy\n",
    "from spacymoji import Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "emoji = Emoji(nlp)\n",
    "nlp.add_pipe(emoji, first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_scrub_text(reviews):\n",
    "    '''\n",
    "    Function to merge text and emoji - utilizes multiprocessing for merge emoji\n",
    "    INPUT:\n",
    "        reviews: array-like, pandas DataFrame column containing review texts\n",
    "    OUTPUT:\n",
    "        lemmatized: pandas DataFrame column with merged texts\n",
    "    '''\n",
    "    lemmatized = []\n",
    "    cpus = cpu_count() - 1\n",
    "    pool = Pool(processes=cpus)\n",
    "    lemmatized = pool.map(merge_emoji, reviews)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "def merge_emoji(text):\n",
    "    '''\n",
    "    Function to merge emoji with text\n",
    "    INPUT:\n",
    "        text: string, text of review\n",
    "        \n",
    "    OUTPUT:\n",
    "        merged text\n",
    "    '''\n",
    "    try:\n",
    "        sen = re.sub(r\"[\\U00010000-\\U0010ffff]\",\"\", text).strip()\n",
    "    except:\n",
    "        print(text)\n",
    "    x = nlp(text)\n",
    "    if x._.has_emoji:\n",
    "        emoji = sen+\" \"+(\" \".join(list(set([item[2] for item in x._.emoji]))))\n",
    "        x = nlp(emoji)\n",
    "    \n",
    "    return x.text.lower()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lem_turn1'] = multi_scrub_text(data['turn1'])\n",
    "data['lem_turn2'] = multi_scrub_text(data['turn2'])\n",
    "data['lem_turn3'] = multi_scrub_text(data['turn3'])\n",
    "data['turn'] = data['lem_turn1']+\" \"+data['lem_turn3'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precess punctuation & mispell words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embeddings_index = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True,limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'don&apos;t':'does not',\n",
    "                \"can&apos;t\":\"can not\",\n",
    "                \"i&apos;m\":\"I am\",\n",
    "                'knw':\"know\",\n",
    "                'yess':\"yes\",\n",
    "                'texte':\"text\",\n",
    "                'humour':\"humor\",\n",
    "                \"frnd\":\"friend\",\n",
    "                \"haa\":\"ha\",\n",
    "                \"darle\":\"darling\",\n",
    "                \"intreste\":\"intresting\",\n",
    "                \"sry\":\"sorry\",\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium',\n",
    "                \"plzz\":\"plz\",\n",
    "                \"thanku\":\"thx\",\n",
    "                \"okkk\":\"ok\",\n",
    "                \"iwant\":\"i want\",\n",
    "                'siri':\"Siri\",\n",
    "                \"doesnt\":\"does not\",\n",
    "                'ohk':\"ok\",\n",
    "                'okk':\"ok\",\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30160/30160 [00:00<00:00, 260922.55it/s]\n",
      "100%|██████████| 30160/30160 [00:00<00:00, 152017.03it/s]\n",
      "100%|██████████| 30160/30160 [00:00<00:00, 135982.98it/s]\n",
      "100%|██████████| 30160/30160 [00:00<00:00, 248762.01it/s]\n",
      "100%|██████████| 30160/30160 [00:00<00:00, 194447.58it/s]\n"
     ]
    }
   ],
   "source": [
    "## Precess the train text\n",
    "data['turn'] = data['turn'].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "data['turn'] = data['turn'].progress_apply(lambda x: clean_text(x))\n",
    "data['turn'] = data['turn'].progress_apply(lambda x: clean_numbers(x))\n",
    "data['turn'] = data['turn'].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = data['turn'].progress_apply(lambda x: x.split())\n",
    "to_remove = ['a','to','of','and','’',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30160/30160 [00:00<00:00, 676211.34it/s]\n",
      "100%|██████████| 30160/30160 [00:00<00:00, 680730.82it/s]\n",
      "100%|██████████| 11556/11556 [00:00<00:00, 459329.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 59.88% of vocab\n",
      "Found embeddings for  97.29% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Check result\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)\n",
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save processed data\n",
    "data.to_csv(\"ProcessedData.csv\",header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ProcessedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 70\n",
    "MAX_NUM_WORDS = 95000\n",
    "tokenizer = Tokenizer(MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(data['turn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_index = tokenizer.texts_to_sequences(data['turn'])\n",
    "texts_index_pad = pad_sequences(texts_index , maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,  29,  60, 123],\n",
       "       [  0,   0,   0, ..., 160, 612,   2],\n",
       "       [  0,   0,   0, ...,  75,   2, 265],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 179,  21, 782],\n",
       "       [  0,   0,   0, ...,   2, 486,  91],\n",
       "       [  0,   0,   0, ..., 193,   9,   3]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_index_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLENEWS_FILE_GLOVE = 'GoogleNews-vectors-negative300.bin'\n",
    "EMBEDDING_LIMIT = 500000\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(GOOGLENEWS_FILE_GLOVE, binary=True,limit=EMBEDDING_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLENEWS_FILE_GLOVE = 'GoogleNews-vectors-negative300.bin'\n",
    "MAX_NUM_WORDS = 95000\n",
    "EMBEDDING_DIM = 300\n",
    "EMBEDDING_LIMIT = 500000\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "def load_and_generate_matrix_embedding(path, word_index):\n",
    "    def get_coefs(word_, *arr):\n",
    "        return word_, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    max_num_words = min(MAX_NUM_WORDS, len(word_index))+1\n",
    "    matrix_embedding = np.zeros((max_num_words, EMBEDDING_DIM))\n",
    "    \n",
    "    embeddings_index = KeyedVectors.load_word2vec_format(GOOGLENEWS_FILE_GLOVE, binary=True,limit=EMBEDDING_LIMIT)\n",
    "    \n",
    "    for word,i in word_index.items():\n",
    "        if i >= max_num_words:\n",
    "                continue\n",
    "        if word in embeddings_index.vocab:\n",
    "            matrix_embedding[i] = embeddings_index.word_vec(word)\n",
    "        else:\n",
    "            matrix_embedding[i] = np.random.randn(EMBEDDING_DIM)\n",
    "            \n",
    "    return matrix_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = load_and_generate_matrix_embedding(GOOGLENEWS_FILE_GLOVE, tokenizer.word_index)\n",
    "max_num_words = min(MAX_NUM_WORDS, len(tokenizer.word_index))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "from keras_self_attention  import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 70, 300)           3460800   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 70, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 70, 256)           439296    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 70, 128)           164352    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 70, 128)           0         \n",
      "_________________________________________________________________\n",
      "seq_self_attention_1 (SeqSel (None, 70, 128)           8257      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8960)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 35844     \n",
      "=================================================================\n",
      "Total params: 4,108,549\n",
      "Trainable params: 4,108,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_num_words,\n",
    "                    EMBEDDING_DIM,\n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                    trainable=True))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True,dropout=0.2,recurrent_dropout=0.2)))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True,recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(SeqSelfAttention(attention_activation='relu'))\n",
    "model.add(Flatten())\n",
    "#model.add(Attention(max_num_words))\n",
    "model.add(Dense(units=4, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(data['label']).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts_index_pad, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xinhan/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 19302 samples, validate on 4826 samples\n",
      "Epoch 1/5\n",
      "19302/19302 [==============================] - 92s 5ms/step - loss: 0.7046 - accuracy: 0.7284 - val_loss: 0.4382 - val_accuracy: 0.8359\n",
      "Epoch 2/5\n",
      "19302/19302 [==============================] - 87s 5ms/step - loss: 0.3752 - accuracy: 0.8678 - val_loss: 0.3602 - val_accuracy: 0.8738\n",
      "Epoch 3/5\n",
      "19302/19302 [==============================] - 83s 4ms/step - loss: 0.2904 - accuracy: 0.8988 - val_loss: 0.3586 - val_accuracy: 0.8751\n",
      "Epoch 4/5\n",
      "19302/19302 [==============================] - 83s 4ms/step - loss: 0.2547 - accuracy: 0.9104 - val_loss: 0.3515 - val_accuracy: 0.8780\n",
      "Epoch 5/5\n",
      "19302/19302 [==============================] - 83s 4ms/step - loss: 0.2244 - accuracy: 0.9232 - val_loss: 0.3607 - val_accuracy: 0.8780\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turn1</th>\n",
       "      <th>turn2</th>\n",
       "      <th>turn3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hmm</td>\n",
       "      <td>What does your bio mean?</td>\n",
       "      <td>I don’t have any bio</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What you like</td>\n",
       "      <td>very little things</td>\n",
       "      <td>Ok</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>How so?</td>\n",
       "      <td>I want to fuck babu</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what did you guess</td>\n",
       "      <td>what what</td>\n",
       "      <td>fuck</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We ?</td>\n",
       "      <td>of course we will!</td>\n",
       "      <td>What gender movies you like??</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                turn1                     turn2  \\\n",
       "0                 Hmm  What does your bio mean?   \n",
       "1       What you like        very little things   \n",
       "2                 Yes                   How so?   \n",
       "3  what did you guess                 what what   \n",
       "4                We ?        of course we will!   \n",
       "\n",
       "                           turn3   label  \n",
       "0           I don’t have any bio  others  \n",
       "1                            Ok   others  \n",
       "2            I want to fuck babu  others  \n",
       "3                           fuck  others  \n",
       "4  What gender movies you like??  others  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpath = \"bun-data//test.txt\"\n",
    "testdata = pd.read_csv(testpath,  sep='\\t', lineterminator='\\r')\n",
    "testdata.drop(\"id\",axis=1,inplace=True)\n",
    "testdata.dropna(how = \"all\",inplace=True)\n",
    "testdata.fillna(\" \",inplace=True)\n",
    "testdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata['lem_turn1'] = multi_scrub_text(testdata['turn1'])\n",
    "testdata['lem_turn2'] = multi_scrub_text(testdata['turn2'])\n",
    "testdata['lem_turn3'] = multi_scrub_text(testdata['turn3'])\n",
    "testdata['turn'] = testdata['lem_turn1']+\". \"+testdata['lem_turn3'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5509/5509 [00:00<00:00, 253635.20it/s]\n",
      "100%|██████████| 5509/5509 [00:00<00:00, 145438.65it/s]\n",
      "100%|██████████| 5509/5509 [00:00<00:00, 138653.22it/s]\n",
      "100%|██████████| 5509/5509 [00:00<00:00, 248643.29it/s]\n"
     ]
    }
   ],
   "source": [
    "testdata['turn'] = testdata['turn'].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "testdata['turn'] = testdata['turn'].progress_apply(lambda x: clean_text(x))\n",
    "testdata['turn'] = testdata['turn'].progress_apply(lambda x: clean_numbers(x))\n",
    "testdata['turn'] = testdata['turn'].progress_apply(lambda x: replace_typical_misspell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = tokenizer.texts_to_sequences(testdata['turn'].values)\n",
    "testX = pad_sequences(testX, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9506183e-04, 3.5937759e-05, 9.9121606e-01, 7.7529722e-03],\n",
       "       [5.3257559e-04, 1.3523049e-03, 9.9630642e-01, 1.8087635e-03],\n",
       "       [8.2888198e-01, 1.4418376e-05, 1.6590360e-01, 5.1999702e-03],\n",
       "       ...,\n",
       "       [8.0465275e-04, 2.4661593e-04, 9.9828571e-01, 6.6293636e-04],\n",
       "       [2.0359247e-03, 5.7735358e-04, 9.8764753e-01, 9.7391680e-03],\n",
       "       [7.8668920e-05, 1.2653698e-04, 9.9843019e-01, 1.3646433e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = pd.get_dummies(testdata['label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8863677618442548"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(map(lambda x: np.argmax(x), testy)), list(map(lambda x: np.argmax(x), y_hat)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
